pip install requests beautifulsoup4 sentence-transformers faiss-cpu transformers


import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import pipeline

# Step 1: Data Ingestion

def scrape_website(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract text and metadata
    title = soup.title.string if soup.title else 'No Title'
    paragraphs = soup.find_all('p')
    content = ' '.join([para.get_text() for para in paragraphs])
    
    return {
        'url': url,
        'title': title,
        'content': content
    }

def chunk_content(content, chunk_size=200):
    return [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]

def embed_chunks(chunks, model):
    return model.encode(chunks)

# Example websites to scrape
urls = [
    "https://www.uchicago.edu/",
    "https://www.washington.edu/",
    "https://www.stanford.edu/",
    "https://und.edu"
]

# Scrape websites and prepare data
data = []
for url in urls:
    scraped_data = scrape_website(url)
    chunks = chunk_content(scraped_data['content'])
    scraped_data['chunks'] = chunks
    data.append(scraped_data)

# Load the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Create a FAISS index
dimension = 384  # Dimension of the embeddings
index = faiss.IndexFlatL2(dimension)

# Add embeddings to the index
for entry in data:
    embeddings = embed_chunks(entry['chunks'], model)
    entry['embeddings'] = embeddings
    index.add(np.array(embeddings).astype('float32'))

# Step 2: Query Handling

def query_embedding(query, model):
    return model.encode([query])

def retrieve_relevant_chunks(query_vec, index, data, k=5):
    D, I = index.search(np.array(query_vec).astype('float32'), k)
    retrieved_chunks = []
    
    # Ensure we only access valid indices
    for i in I[0]:
        if i < len(data):  # Check if index is within bounds
            retrieved_chunks.extend(data[i]['chunks'])
    
    return retrieved_chunks

# Step 3: Response Generation

# Load the text generation model
generator = pipeline('text-generation', model='distilgpt2')

def generate_response(retrieved_chunks, user_query):
    context = " ".join(retrieved_chunks)
    prompt = f"Based on the following information, answer the question: {user_query}\n\n{context}"
    
    response = generator(prompt, max_length=150, num_return_sequences=1)
    return response[0]['generated_text']

# Example user query
user_query = "What programs does the University of Chicago offer?"
query_vec = query_embedding(user_query, model)
retrieved_chunks = retrieve_relevant_chunks(query_vec, index, data)
response = generate_response(retrieved_chunks, user_query)

print("User  Query:", user_query)
print("Response:", response)
